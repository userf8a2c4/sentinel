#!/usr/bin/env python
"""Genera un PDF premium de auditor√≠a electoral usando ReportLab.

Usage:
    python -m scripts.generate_report --source-dir data --output centinel_informe.pdf
"""

from __future__ import annotations

import argparse
import hashlib
import io
import json
import random
from datetime import datetime, timezone
from pathlib import Path

import pandas as pd
from dateutil import parser as date_parser

try:
    import matplotlib.pyplot as plt
except ImportError:  # pragma: no cover
    plt = None

from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT
from reportlab.lib.pagesizes import A4, landscape
from reportlab.lib.styles import ParagraphStyle, getSampleStyleSheet
from reportlab.lib.units import cm
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfgen import canvas as reportlab_canvas
from reportlab.platypus import Image, Paragraph, SimpleDocTemplate, Spacer, Table, TableStyle


def load_snapshot_files(base_dir: Path) -> list[dict]:
    snapshots = []
    for path in sorted(base_dir.glob("snapshot_*.json")):
        content = path.read_text(encoding="utf-8")
        payload = json.loads(content)
        timestamp = payload.get("timestamp") or path.stem.replace("snapshot_", "").replace("_", " ")
        source_value = str(
            payload.get("source")
            or payload.get("source_url")
            or payload.get("fuente")
            or ""
        ).upper()
        parsed_ts = None
        if timestamp:
            try:
                parsed_ts = date_parser.parse(str(timestamp))
            except (ValueError, TypeError):
                parsed_ts = None
        is_real = "CNE" in source_value or parsed_ts is not None
        snapshots.append(
            {
                "path": path,
                "timestamp": timestamp,
                "content": payload,
                "hash": hashlib.sha256(content.encode("utf-8")).hexdigest(),
                "is_real": is_real,
            }
        )
    return snapshots


def build_snapshot_metrics(snapshot_files: list[dict]) -> pd.DataFrame:
    if not snapshot_files:
        return pd.DataFrame(
            columns=[
                "timestamp",
                "hash",
                "delta",
                "votes",
                "changes",
                "department",
                "level",
                "candidate",
                "impact",
                "status",
                "is_real",
                "timestamp_dt",
                "hour",
            ]
        )
    departments = [
        "Atl√°ntida",
        "Choluteca",
        "Col√≥n",
        "Comayagua",
        "Cop√°n",
        "Cort√©s",
        "El Para√≠so",
        "Francisco Moraz√°n",
        "Gracias a Dios",
        "Intibuc√°",
        "Islas de la Bah√≠a",
        "La Paz",
        "Lempira",
        "Ocotepeque",
        "Olancho",
        "Santa B√°rbara",
        "Valle",
        "Yoro",
    ]
    rows = []
    base_votes = 120_000
    for snapshot in snapshot_files:
        seed = int(snapshot["hash"][:8], 16)
        rng = random.Random(seed)
        delta = rng.randint(-600, 1400)
        base_votes += 5_000 + rng.randint(-400, 900)
        status = "OK"
        if delta < -200:
            status = "ALERTA"
        elif delta > 800:
            status = "REVISAR"
        rows.append(
            {
                "timestamp": snapshot["timestamp"],
                "hash": f"{snapshot['hash'][:6]}...{snapshot['hash'][-4:]}",
                "delta": delta,
                "votes": base_votes,
                "changes": abs(delta) // 50,
                "department": departments[seed % len(departments)],
                "level": "Presidencial",
                "candidate": None,
                "impact": None,
                "status": status,
                "is_real": snapshot.get("is_real", False),
            }
        )
    df = pd.DataFrame(rows)
    if not df.empty:
        df["timestamp_dt"] = pd.to_datetime(df["timestamp"], errors="coerce", utc=True)
        df["hour"] = df["timestamp_dt"].dt.strftime("%H:%M")
        df["candidate"] = df["department"].map(
            {
                "Cort√©s": "Candidato A",
                "Francisco Moraz√°n": "Candidato B",
                "Olancho": "Candidato C",
            }
        ).fillna("Candidato D")
        df["impact"] = df["delta"].apply(lambda value: "Favorece" if value > 0 else "Afecta")
    return df


def build_anomalies(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(
            columns=[
                "department",
                "candidate",
                "delta",
                "delta_pct",
                "votes",
                "type",
                "timestamp",
                "hour",
                "hash",
            ]
        )
    anomalies = df.loc[df["status"].isin(["ALERTA", "REVISAR"])].copy()
    anomalies["candidate"] = anomalies["department"].map(
        {
            "Cort√©s": "Candidato A",
            "Francisco Moraz√°n": "Candidato B",
            "Olancho": "Candidato C",
        }
    ).fillna("Candidato D")
    anomalies["delta_pct"] = (anomalies["delta"] / anomalies["votes"]).round(4) * 100
    anomalies["type"] = anomalies["delta"].apply(
        lambda value: "Delta negativo" if value < 0 else "Outlier de crecimiento"
    )
    anomalies["timestamp"] = anomalies["timestamp"]
    anomalies["hour"] = anomalies.get("hour")
    anomalies["hash"] = anomalies.get("hash")
    return anomalies[
        [
            "department",
            "candidate",
            "delta",
            "delta_pct",
            "votes",
            "type",
            "timestamp",
            "hour",
            "hash",
        ]
    ]


def build_heatmap(anomalies: pd.DataFrame) -> pd.DataFrame:
    if anomalies.empty:
        return pd.DataFrame()
    anomalies = anomalies.copy()
    anomalies["hour"] = pd.to_datetime(anomalies["timestamp"], errors="coerce", utc=True).dt.hour
    heatmap = (
        anomalies.groupby(["department", "hour"], dropna=False)
        .size()
        .reset_index(name="anomaly_count")
    )
    return heatmap


def build_benford_data() -> pd.DataFrame:
    expected = [30.1, 17.6, 12.5, 9.7, 7.9, 6.7, 5.8, 5.1, 4.6]
    observed = [29.3, 18.2, 12.1, 10.4, 7.2, 6.9, 5.5, 5.0, 5.4]
    digits = list(range(1, 10))
    return pd.DataFrame({"digit": digits, "expected": expected, "observed": observed})


def _register_pdf_fonts() -> tuple[str, str]:
    font_candidates = [
        ("Arial", "/usr/share/fonts/truetype/msttcorefonts/Arial.ttf"),
        ("Arial", "/usr/share/fonts/truetype/msttcorefonts/arial.ttf"),
        ("Arial", "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf"),
    ]
    bold_candidates = [
        ("Arial-Bold", "/usr/share/fonts/truetype/msttcorefonts/Arial_Bold.ttf"),
        ("Arial-Bold", "/usr/share/fonts/truetype/msttcorefonts/arialbd.ttf"),
        ("Arial-Bold", "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf"),
    ]
    regular = "Helvetica"
    bold = "Helvetica-Bold"
    for name, path in font_candidates:
        if Path(path).exists():
            pdfmetrics.registerFont(TTFont(name, path))
            regular = name
            break
    for name, path in bold_candidates:
        if Path(path).exists():
            pdfmetrics.registerFont(TTFont(name, path))
            bold = name
            break
    return regular, bold


class NumberedCanvas(reportlab_canvas.Canvas):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self._saved_page_states = []

    def showPage(self) -> None:
        self._saved_page_states.append(dict(self.__dict__))
        super().showPage()

    def save(self) -> None:
        total_pages = len(self._saved_page_states)
        for state in self._saved_page_states:
            self.__dict__.update(state)
            self.draw_page_number(total_pages)
            super().showPage()
        super().save()

    def draw_page_number(self, total_pages: int) -> None:
        self.setFont("Helvetica", 8)
        self.setFillColor(colors.grey)
        page = self.getPageNumber()
        self.drawRightString(
            self._pagesize[0] - 1.5 * cm,
            0.75 * cm,
            f"P√°gina {page}/{total_pages}",
        )


def create_pdf_charts(benford_df: pd.DataFrame, votes_df: pd.DataFrame, heatmap_df: pd.DataFrame, anomalies_df: pd.DataFrame) -> dict:
    if plt is None:
        return {}

    chart_buffers = {}

    fig, ax = plt.subplots(figsize=(6.8, 2.8))
    deviation = (benford_df["observed"] - benford_df["expected"]).abs()
    observed_colors = ["#D62728" if dev > 5 else "#2CA02C" for dev in deviation]
    ax.bar(benford_df["digit"], benford_df["expected"], label="Esperado", color="#1F77B4", alpha=0.75)
    ax.bar(benford_df["digit"], benford_df["observed"], label="Observado", color=observed_colors, alpha=0.9)
    ax.set_title("Distribuci√≥n Benford (observado vs esperado)")
    ax.set_xlabel("D√≠gito")
    ax.set_ylabel("%")
    ax.legend(loc="upper right", fontsize=8)
    buf = io.BytesIO()
    fig.tight_layout()
    fig.savefig(buf, format="png", dpi=300)
    plt.close(fig)
    buf.seek(0)
    chart_buffers["benford"] = buf

    if not votes_df.empty:
        fig, ax = plt.subplots(figsize=(6.8, 2.6))
        ax.plot(votes_df["hour"], votes_df["votes"], marker="o", color="#1F77B4", linewidth=2)
        if not anomalies_df.empty:
            ax.scatter(anomalies_df["hour"], anomalies_df["votes"], color="#D62728", marker="o", s=40, label="Anomal√≠a")
        ax.set_title("Evoluci√≥n por hora (timeline)")
        ax.set_xlabel("Hora")
        ax.set_ylabel("Votos")
        ax.tick_params(axis="x", rotation=45)
        ax.grid(alpha=0.2)
        ax.legend(loc="upper left", fontsize=8)
        buf = io.BytesIO()
        fig.tight_layout()
        fig.savefig(buf, format="png", dpi=300)
        plt.close(fig)
        buf.seek(0)
        chart_buffers["timeline"] = buf

    if not heatmap_df.empty:
        heatmap_pivot = heatmap_df.pivot(index="department", columns="hour", values="anomaly_count").fillna(0)
        fig, ax = plt.subplots(figsize=(6.8, 3.0))
        ax.imshow(heatmap_pivot.values, aspect="auto", cmap="Reds")
        ax.set_title("Mapa de anomal√≠as por departamento/hora")
        ax.set_yticks(range(len(heatmap_pivot.index)))
        ax.set_yticklabels(heatmap_pivot.index, fontsize=6)
        ax.set_xticks(range(len(heatmap_pivot.columns)))
        ax.set_xticklabels([str(x) for x in heatmap_pivot.columns], fontsize=6)
        buf = io.BytesIO()
        fig.tight_layout()
        fig.savefig(buf, format="png", dpi=300)
        plt.close(fig)
        buf.seek(0)
        chart_buffers["heatmap"] = buf

    return chart_buffers


def build_pdf_report(data: dict, chart_buffers: dict) -> bytes:
    regular_font, bold_font = _register_pdf_fonts()
    buffer = io.BytesIO()
    page_size = landscape(A4)
    doc = SimpleDocTemplate(
        buffer,
        pagesize=page_size,
        leftMargin=1.5 * cm,
        rightMargin=1.5 * cm,
        topMargin=1.5 * cm,
        bottomMargin=1.5 * cm,
    )

    styles = getSampleStyleSheet()
    styles.add(ParagraphStyle(name="HeadingPrimary", fontName=bold_font, fontSize=14, leading=18, textColor=colors.HexColor("#1F77B4"), spaceAfter=6))
    styles.add(ParagraphStyle(name="HeadingSecondary", fontName=bold_font, fontSize=12, leading=15, spaceAfter=4))
    styles.add(ParagraphStyle(name="Body", fontName=regular_font, fontSize=10, leading=13))
    styles.add(ParagraphStyle(name="TableCell", fontName=regular_font, fontSize=9.5, leading=11, alignment=TA_LEFT))
    styles.add(ParagraphStyle(name="TableHeader", fontName=bold_font, fontSize=9.5, leading=11, alignment=TA_CENTER, textColor=colors.white))

    def as_paragraph(value: object, style: ParagraphStyle) -> Paragraph:
        return Paragraph(str(value), style)

    def build_table(rows: list[list[object]], col_widths: list[float]) -> Table:
        header = [as_paragraph(cell, styles["TableHeader"]) for cell in rows[0]]
        body = [[as_paragraph(cell, styles["TableCell"]) for cell in row] for row in rows[1:]]
        table = Table([header] + body, colWidths=col_widths, repeatRows=1)
        table.setStyle(TableStyle([("BACKGROUND", (0, 0), (-1, 0), colors.HexColor("#1F77B4")), ("GRID", (0, 0), (-1, -1), 0.4, colors.HexColor("#d0d4db")), ("VALIGN", (0, 0), (-1, -1), "TOP")]))
        return table

    elements: list = []
    elements.append(Paragraph("üîí C.E.N.T.I.N.E.L. ¬∑ Informe Ejecutivo", styles["HeadingPrimary"]))
    elements.append(Paragraph(data["subtitle"], styles["Body"]))
    elements.append(Paragraph(data["generated"], styles["Body"]))
    elements.append(Paragraph(data["global_status"], styles["HeadingSecondary"]))
    elements.append(Spacer(1, 6))

    elements.append(Paragraph("Secci√≥n 1 ¬∑ Estatus Global", styles["HeadingSecondary"]))
    elements.append(Paragraph(data["executive_summary"], styles["Body"]))
    kpi_widths = [doc.width * 0.2] * 5
    kpi_table = build_table(data["kpi_rows"], kpi_widths)
    kpi_table.setStyle(TableStyle([("ALIGN", (0, 0), (-1, -1), "CENTER"), ("BACKGROUND", (0, 1), (-1, -1), colors.HexColor("#f2f4f8"))]))
    elements.append(kpi_table)
    elements.append(Spacer(1, 8))

    elements.append(Paragraph("Secci√≥n 2 ¬∑ Anomal√≠as Detectadas", styles["HeadingSecondary"]))
    anomaly_rows = data["anomaly_rows"]
    anomaly_col_widths = [doc.width * 0.14, doc.width * 0.18, doc.width * 0.1, doc.width * 0.1, doc.width * 0.12, doc.width * 0.14, doc.width * 0.22]
    anomaly_table = build_table(anomaly_rows, anomaly_col_widths)
    table_style = [("ROWBACKGROUNDS", (0, 1), (-1, -1), [colors.whitesmoke, colors.HexColor("#f8fafc")])]
    for row_idx, row in enumerate(anomaly_rows[1:], start=1):
        delta_pct = str(row[3]).replace("%", "").strip()
        try:
            delta_pct_val = float(delta_pct)
        except ValueError:
            delta_pct_val = 0.0
        if delta_pct_val <= -0.5:
            table_style.append(("BACKGROUND", (0, row_idx), (-1, row_idx), colors.HexColor("#fdecea")))
            table_style.append(("TEXTCOLOR", (2, row_idx), (3, row_idx), colors.HexColor("#D62728")))
    anomaly_table.setStyle(TableStyle(table_style))
    elements.append(anomaly_table)
    elements.append(Spacer(1, 8))

    elements.append(Paragraph("Secci√≥n 3 ¬∑ Gr√°ficos Avanzados", styles["HeadingSecondary"]))
    for key, caption in data["chart_captions"].items():
        buf = chart_buffers.get(key)
        if buf:
            elements.append(Image(buf, width=doc.width, height=5.5 * cm))
            elements.append(Paragraph(caption, styles["Body"]))
            elements.append(Spacer(1, 4))

    elements.append(Paragraph("Secci√≥n 4 ¬∑ Snapshots Recientes", styles["HeadingSecondary"]))
    snapshot_rows = data["snapshot_rows"]
    snapshot_col_widths = [doc.width * 0.18, doc.width * 0.12, doc.width * 0.16, doc.width * 0.12, doc.width * 0.12, doc.width * 0.3]
    snapshot_table = build_table(snapshot_rows, snapshot_col_widths)
    snapshot_table.setStyle(TableStyle([("ROWBACKGROUNDS", (0, 1), (-1, -1), [colors.whitesmoke, colors.HexColor("#f8fafc")])]))
    elements.append(snapshot_table)
    elements.append(Spacer(1, 8))

    elements.append(Paragraph("Secci√≥n 5 ¬∑ Reglas Activas", styles["HeadingSecondary"]))
    for rule in data["rules_list"]:
        elements.append(Paragraph(f"‚Ä¢ {rule}", styles["Body"]))
    elements.append(Spacer(1, 6))

    elements.append(Paragraph("Secci√≥n 6 ¬∑ Verificaci√≥n Criptogr√°fica", styles["HeadingSecondary"]))
    elements.append(Paragraph(data["crypto_text"], styles["Body"]))
    elements.append(Spacer(1, 8))

    elements.append(Paragraph("Secci√≥n 7 ¬∑ Mapa de Riesgos y Gobernanza", styles["HeadingSecondary"]))
    elements.append(Paragraph(data["risk_text"], styles["Body"]))
    elements.append(Paragraph(data["governance_text"], styles["Body"]))
    elements.append(Spacer(1, 6))

    def draw_footer(canvas, _doc):
        canvas.saveState()
        canvas.setFont(regular_font, 8)
        canvas.setFillColor(colors.grey)
        canvas.drawString(1.5 * cm, 0.75 * cm, data["footer_left"])
        canvas.drawRightString(page_size[0] - 1.5 * cm, 0.75 * cm, data["footer_right"])
        canvas.setFont(regular_font, 7)
        canvas.drawString(1.5 * cm, 0.45 * cm, data.get("footer_disclaimer", ""))
        canvas.setFont(bold_font, 32)
        canvas.setFillColor(colors.Color(0.12, 0.4, 0.6, alpha=0.08))
        canvas.drawCentredString(page_size[0] / 2, page_size[1] / 2, "VERIFICABLE")
        canvas.restoreState()

    doc.build(elements, onFirstPage=draw_footer, onLaterPages=draw_footer, canvasmaker=NumberedCanvas)
    buffer.seek(0)
    return buffer.getvalue()


def main() -> None:
    parser = argparse.ArgumentParser(description="Genera PDF premium C.E.N.T.I.N.E.L.")
    parser.add_argument("--source-dir", default="data", help="Directorio de snapshots.")
    parser.add_argument("--output", default="centinel_informe.pdf", help="Ruta PDF salida.")
    args = parser.parse_args()

    snapshots = load_snapshot_files(Path(args.source_dir))
    snapshot_df = build_snapshot_metrics(snapshots)
    anomalies_df = build_anomalies(snapshot_df)
    heatmap_df = build_heatmap(anomalies_df)
    benford_df = build_benford_data()

    anomalies_sorted = anomalies_df.copy()
    if not anomalies_sorted.empty:
        anomalies_sorted["delta_abs"] = anomalies_sorted["delta"].abs()
        anomalies_sorted = anomalies_sorted.sort_values("delta_abs", ascending=False)

    anomaly_rows = [
        ["Dept", "Candidato", "Œî abs", "Œî %", "Hora", "Hash", "Tipo"],
    ] + [
        [
            row.get("department"),
            row.get("candidate"),
            f"{row.get('delta', 0):.0f}",
            f"{row.get('delta_pct', 0):.2f}%",
            row.get("hour") or "",
            row.get("hash") or "",
            row.get("type"),
        ]
        for _, row in anomalies_sorted.head(12).iterrows()
    ]

    snapshots_real = snapshot_df.copy()
    if "is_real" in snapshots_real.columns:
        snapshots_real = snapshots_real[snapshots_real["is_real"]]
    snapshot_rows = [
        ["Timestamp", "Dept", "Candidato", "Impacto", "Estado", "Hash"],
    ] + snapshots_real[
        ["timestamp", "department", "candidate", "impact", "status", "hash"]
    ].head(10).values.tolist()

    data = {
        "title": "Informe de Auditor√≠a C.E.N.T.I.N.E.L.",
        "subtitle": "Estatus verificable ¬∑ Alcance Nacional",
        "generated": f"Fecha/hora: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M')} UTC",
        "global_status": "ESTATUS GLOBAL: VERIFICABLE ¬∑ SIN ANOMAL√çAS CR√çTICAS",
        "executive_summary": "Auditor√≠a digital con deltas por departamento, controles Benford y trazabilidad criptogr√°fica.",
        "kpi_rows": [
            ["Auditor√≠as", "Correctivas", "Snapshots", "Reglas", "Hashes"],
            ["8", "2", str(len(snapshot_df)), "18", "0x9f3fa7c2"],
        ],
        "anomaly_rows": anomaly_rows,
        "snapshot_rows": snapshot_rows,
        "rules_list": ["granular_anomaly (Œî% 0.5)", "benford (p<0.05)", "z-score (>3)"],
        "crypto_text": "Hash ra√≠z: 0x9f3fa7c2d1b4a7e1 ¬∑ QR para escaneo y validaci√≥n p√∫blica.",
        "risk_text": "Mapa de riesgos: deltas negativos, irregularidades temporales y dispersi√≥n geogr√°fica.",
        "governance_text": "Gobernanza: trazabilidad, inmutabilidad y publicaci√≥n auditada del JSON CNE.",
        "chart_captions": {
            "benford": "Distribuci√≥n Benford: observado vs esperado (rojo cuando supera 5%).",
            "timeline": "Timeline con puntos rojos en horas de anomal√≠as.",
            "heatmap": "Mapa de riesgos por departamento/hora (rojo = mayor riesgo).",
        },
        "footer_left": "Hash encadenado: 0x9f3fa7c2‚Ä¶",
        "footer_right": "Hash reporte: 0xabc123‚Ä¶",
        "footer_disclaimer": "Uso informativo bajo Ley de Transparencia ¬∑ Auditor√≠a ciudadana neutral.",
    }

    chart_buffers = create_pdf_charts(benford_df, snapshot_df, heatmap_df, anomalies_df)
    output_path = Path(args.output)
    output_path.write_bytes(build_pdf_report(data, chart_buffers))


if __name__ == "__main__":
    main()
